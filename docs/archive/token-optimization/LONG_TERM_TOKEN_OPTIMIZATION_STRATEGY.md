# Long-Term Token Optimization Strategy for Synapse
**Created**: 2025-10-18
**Purpose**: Comprehensive guide to minimize token costs as project scales

---

## Executive Summary

Your Synapse project is well-positioned for token efficiency:
- ‚úÖ **Phase 1 Complete**: 38% reduction (2,000 tokens/call saved)
- üéØ **Optimization Potential**: Additional 50-70% savings available
- üí∞ **Long-term Impact**: At scale, could save $1000s/month

This document provides a comprehensive, prioritized roadmap for token optimization that will compound as your project grows.

---

## Current State Analysis

### Token Breakdown (Per Interaction)

| Component | Tokens | % of Total | Optimization Potential |
|-----------|--------|------------|----------------------|
| **System Prompt** | 400 | 12% | ‚úÖ Optimized |
| **Tool Schemas** | 2,700 | 84% | ‚ö†Ô∏è High (20-30% reducible) |
| **Task Data (46 tasks)** | 4,600-9,200 | Variable | ‚ö†Ô∏è Very High (50-70% reducible) |
| **Message History** | 50-100/turn | Accumulates | ‚ö†Ô∏è Medium (summarization) |
| **Knowledge Queries** | 500-1,500 | On-demand | ‚ö†Ô∏è Medium (indexing) |

**Total per interaction**: ~3,200-12,000 tokens (depending on task data)

---

## Optimization Strategy: 5 Tiers

### üü¢ TIER 1: Quick Wins (1-2 hours, 15-25% additional savings)
**Impact**: High
**Effort**: Low
**When**: Implement now

#### 1.1 Compress Tool Docstrings (600 tokens saved)

**Current Problem**: Your tool schemas include verbose docstrings that get sent on EVERY API call.

**Example** (from `make_actionable`):
```python
def make_actionable(
    self,
    task_id: str,
    location: Literal["home", "house", "yard", "errand", "bunnings", "parents"],
    activity: Literal["chore", "maintenance", "call", "email", "computer"],
    # ... more params
) -> str:
    """
    Move task to Next Actions with location and activity context.

    This is the GTD-native tool for processing inbox tasks into actionable items.
    It enforces proper workflow by constraining context choices to valid enums.

    Location specifies WHERE the task happens (physical or virtual context).
    Activity specifies WHAT TYPE of action is required.

    Implementation details:
    - Validates task exists
    - Moves to Next Actions project
    - Adds both location and activity labels
    - Preserves priority, due dates, etc.

    Args:
        task_id: The ID of the task to make actionable
        location: WHERE task happens (home, house, yard, errand, bunnings, parents)
        activity: WHAT type of action (chore, maintenance, call, email, computer)

    Returns:
        JSON with status and task details
    """
```

**Optimized** (move implementation details to comments):
```python
def make_actionable(
    self,
    task_id: str,
    location: Literal["home", "house", "yard", "errand", "bunnings", "parents"],
    activity: Literal["chore", "maintenance", "call", "email", "computer"],
) -> str:
    """Move inbox task to Next Actions with location/activity context."""
    # Implementation: Validates task, moves to Next Actions project,
    # adds labels, preserves priority/dates
```

**Savings**: ~15-25 tokens per tool √ó 25 tools = **~600 tokens**

**Action**:
1. Edit all tool docstrings in `core/agents/todoist.py`
2. Keep only user-facing description (1 line)
3. Move implementation notes to code comments

---

#### 1.2 Task Data Compression (2,300-4,600 tokens saved)

**Current Problem**: `list_tasks()` returns full Todoist API objects with redundant fields.

**Current format** (~100-200 tokens per task):
```json
{
  "id": "8193847562",
  "content": "Buy groceries",
  "description": "",
  "project_id": "2203306141",
  "section_id": null,
  "parent_id": null,
  "order": 1,
  "priority": 1,
  "due": null,
  "labels": ["next", "errand"],
  "created_at": "2025-10-15T14:32:00.000000Z",
  "creator_id": "12345678",
  "assignee_id": null,
  "assigner_id": null,
  "is_completed": false,
  "url": "https://todoist.com/showTask?id=8193847562",
  "duration": null,
  "comments_count": 0
}
```

**Optimized format** (~30-50 tokens per task):
```json
{
  "id": "8193847562",
  "txt": "Buy groceries",
  "lbl": ["next", "errand"],
  "p": 1,
  "crt": "2025-10-15"
}
```

**Savings**: ~100 tokens √ó 46 tasks = **~4,600 tokens** (50-70% reduction in task data)

**Implementation**:
```python
def _compress_task(self, task: Task) -> dict:
    """Compress task to minimal representation for API calls."""
    compressed = {
        "id": task.id,
        "txt": task.content,
    }

    # Only include non-empty fields
    if task.labels:
        compressed["lbl"] = task.labels
    if task.priority > 1:
        compressed["p"] = task.priority
    if task.due:
        compressed["due"] = task.due.date  # Date only, not timestamp
    if task.created_at:
        compressed["crt"] = task.created_at[:10]  # Date only

    return compressed

def list_tasks(self, project_name: str = None, **kwargs) -> str:
    """List tasks (compressed for token efficiency)."""
    tasks = self._get_tasks_list(...)  # Current implementation

    # Compress before returning
    compressed_tasks = [self._compress_task(t) for t in tasks]

    return self._success({
        "tasks": compressed_tasks,
        "count": len(compressed_tasks)
    })
```

**Add to system prompt**:
```yaml
system_prompt: >
  ...
  # TASK DATA FORMAT (compressed for efficiency)
  Tasks returned use abbreviated keys:
  - txt: content
  - lbl: labels
  - p: priority (only if > 1)
  - due: due date
  - crt: created date
```

**Critical**: Test extensively - this changes data format LLM sees.

---

#### 1.3 Remove Unused Tool Schemas (300-500 tokens saved)

**Observation**: Your `todoist.yaml` lists 25 tools, but some are rarely used.

**Strategy**: Create specialized agent variants for different workflows:

```yaml
# agents/todoist_inbox.yaml (inbox processing only)
tools:
  - get_current_time
  - query_knowledge
  - process_inbox       # Wizard handles everything
  - list_tasks          # Only for context
  - complete_task       # For quick 'd' command

# agents/todoist_review.yaml (weekly review)
tools:
  - get_current_time
  - query_knowledge
  - list_tasks
  - update_task
  - move_task
  - review_tasks_without_next_actions

# agents/todoist_full.yaml (current full set)
tools: [all 25 tools]
```

**Usage**:
```bash
./synapse todoist_inbox    # 5 tools (~500 tokens)
./synapse todoist_review   # 7 tools (~900 tokens)
./synapse todoist_full     # 25 tools (~2,700 tokens)
```

**Savings**: ~2,200 tokens when using specialized agents

---

### üü° TIER 2: Medium-Effort Optimizations (3-5 hours, 10-20% additional savings)

#### 2.1 Implement Prompt Caching Strategically

**Current State**: You have 97% cache hit rate - excellent!

**Optimization**: Structure prompts to maximize cache efficiency.

**Strategy**:
```python
# Cache-optimized message structure
messages = [
    # CACHE BOUNDARY 1: System prompt (never changes)
    {"role": "system", "content": system_prompt},

    # CACHE BOUNDARY 2: Tool schemas (rarely change)
    {"role": "system", "content": json.dumps(tools)},

    # CACHE BOUNDARY 3: Recent context (changes per session)
    {"role": "user", "content": "Current time: 2025-10-18 10:30:00"},
    {"role": "user", "content": "Knowledge: [learned rules]"},

    # NON-CACHED: Current conversation (changes every turn)
    *conversation_messages
]
```

**OpenAI Prompt Caching**: Caches last 1024+ tokens of prompt prefix.
**Anthropic Prompt Caching**: Explicit cache control with `cache_control` parameter.

**Benefit**: Reduces input token costs by 50-90% on cached portions.

---

#### 2.2 Knowledge Base Indexing (300-600 tokens saved)

**Current**: `query_knowledge()` returns full markdown files.

**Optimized**: Add semantic chunking with summaries.

**Structure**:
```
knowledge/todoistagent/
‚îú‚îÄ‚îÄ index.json          # Metadata index
‚îú‚îÄ‚îÄ learned_rules.md    # Full content
‚îî‚îÄ‚îÄ learned_rules.chunks.json  # Chunked with embeddings
```

**Index format**:
```json
{
  "learned_rules": {
    "chunks": [
      {
        "id": "lr_001",
        "summary": "Grocery shopping patterns",
        "tokens": 45,
        "content": "User prefers to batch groceries..."
      },
      {
        "id": "lr_002",
        "summary": "Meeting scheduling preferences",
        "tokens": 38,
        "content": "Schedule meetings in morning..."
      }
    ],
    "total_tokens": 1200
  }
}
```

**Implementation**:
```python
def query_knowledge(self, topic: str, search_query: str = None) -> str:
    """Query knowledge base with optional semantic search."""

    if search_query:
        # Return only relevant chunks
        chunks = self._search_knowledge_chunks(topic, search_query)
        return "\n\n".join(c["content"] for c in chunks[:3])  # Top 3 chunks
    else:
        # Return summary + full content
        index = self._load_knowledge_index(topic)
        summary = "\n".join(f"- {c['summary']}" for c in index["chunks"])
        return f"# {topic}\n\n## Summary\n{summary}\n\n## Full Content\n{full_content}"
```

**Savings**: Return 150-300 tokens instead of 500-1,500 for targeted queries.

---

#### 2.3 Message History Summarization (500-1,000 tokens saved)

**Problem**: Message history grows linearly with conversation length.

**Strategy**: Summarize old messages after N turns.

**Implementation**:
```python
def _summarize_old_messages(messages: list, keep_recent: int = 5) -> list:
    """Summarize old conversation, keep recent messages intact."""

    if len(messages) <= keep_recent:
        return messages

    # Keep system prompt and recent messages
    system_messages = [m for m in messages if m["role"] == "system"]
    recent_messages = messages[-keep_recent:]
    old_messages = messages[len(system_messages):-keep_recent]

    # Summarize old messages
    if old_messages:
        summary = _create_summary(old_messages)  # Use LLM to summarize
        summary_message = {
            "role": "system",
            "content": f"[Previous conversation summary]: {summary}"
        }
        return system_messages + [summary_message] + recent_messages

    return messages
```

**Trigger**: After 10 conversation turns, or when messages exceed 5,000 tokens.

**Savings**: ~50-100 tokens per old message removed.

---

### üü† TIER 3: Advanced Optimizations (5-10 hours, 15-25% additional savings)

#### 3.1 Streaming Responses (Perceived Performance + Token Efficiency)

**Current**: Wait for full response, then display.

**Optimized**: Stream tokens as they arrive, abort early if needed.

**Implementation**:
```python
# In core/providers/openai_provider.py
def send_message_streaming(self, client, messages, system_prompt, model, tools):
    """Stream response tokens as they arrive."""

    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": system_prompt}] + messages,
        tools=tools,
        stream=True
    )

    full_response = ""
    for chunk in stream:
        delta = chunk.choices[0].delta
        if delta.content:
            print(delta.content, end="", flush=True)
            full_response += delta.content

    return full_response
```

**Benefits**:
- **Perceived speed**: User sees response immediately
- **Early abort**: Can interrupt if response is off-track
- **Token savings**: Stop generation early if not needed

---

#### 3.2 Tool Result Compression (200-400 tokens saved)

**Current**: Tool results are verbose JSON.

**Example** (current):
```json
{
  "status": "success",
  "message": "Task created successfully",
  "data": {
    "task": {
      "id": "8193847562",
      "content": "Buy groceries",
      "project_id": "2203306141",
      "labels": ["next", "errand"],
      "priority": 1,
      "due": null,
      "created_at": "2025-10-15T14:32:00.000000Z"
    }
  }
}
```

**Optimized**:
```json
{
  "ok": true,
  "msg": "Created",
  "task": {"id": "8193847562", "txt": "Buy groceries"}
}
```

**Implementation**:
```python
def _success(self, data: dict = None, message: str = "") -> str:
    """Return compressed success response."""
    result = {"ok": True}
    if message:
        result["msg"] = message[:50]  # Truncate long messages
    if data:
        result.update(data)  # Merge data directly
    return json.dumps(result)

def _error(self, error_type: str, message: str) -> str:
    """Return compressed error response."""
    return json.dumps({
        "ok": False,
        "err": error_type,
        "msg": message[:100]
    })
```

**Savings**: ~20-40 tokens per tool result √ó 10 tools/interaction = **~400 tokens**

---

#### 3.3 Smart Context Windowing (1,000-2,000 tokens saved)

**Concept**: Only include relevant context for each request.

**Current**: All tools available, all history included.

**Optimized**: Dynamically adjust context based on request type.

**Implementation**:
```python
def _get_relevant_tools(self, user_message: str, all_tools: list) -> list:
    """Return only tools relevant to user's message."""

    message_lower = user_message.lower()

    # Always include core tools
    core_tools = ["get_current_time", "query_knowledge"]

    # Conditional tool loading based on keywords
    if "inbox" in message_lower or "process" in message_lower:
        return core_tools + ["process_inbox", "list_tasks", "make_actionable"]

    elif "remind" in message_lower:
        return core_tools + ["set_reminder", "create_standalone_reminder"]

    elif "review" in message_lower:
        return core_tools + ["list_tasks", "review_tasks_without_next_actions"]

    else:
        # Full tool set for ambiguous requests
        return all_tools
```

**Benefit**: Reduce tool schemas from 2,700 to 500-1,000 tokens for focused tasks.

**Risk**: LLM might not have right tool for edge cases. Fallback to full set if tool call fails.

---

### üî¥ TIER 4: Architectural Changes (10-20 hours, 20-30% additional savings)

#### 4.1 Multi-Model Strategy (Cost + Performance)

**Current**: Single model (gpt-4o or claude-sonnet-4.5) for all tasks.

**Optimized**: Route to different models based on complexity.

**Model Tier Strategy**:

| Task Type | Model | Cost | Use Case |
|-----------|-------|------|----------|
| **Simple** | gpt-4o-mini | $0.15/1M tokens | Quick queries, simple CRUD |
| **Medium** | gpt-4o | $2.50/1M tokens | Inbox processing, scheduling |
| **Complex** | claude-sonnet-4.5 | $3.00/1M tokens | Weekly reviews, learning |

**Implementation**:
```python
def _select_model(self, task_type: str, complexity: str) -> str:
    """Select optimal model based on task complexity."""

    if complexity == "simple":
        return "gpt-4o-mini"  # 90% cheaper than gpt-4o
    elif complexity == "medium":
        return "gpt-4o"
    else:
        return "claude-sonnet-4.5"

# Auto-detect complexity
def chat(agent_name: str):
    user_message = get_user_input()

    # Simple keyword detection
    simple_keywords = ["list", "show", "get", "find", "what"]
    if any(kw in user_message.lower().split()[:2] for kw in simple_keywords):
        model = "gpt-4o-mini"
    else:
        model = agent.model  # Default

    response = provider.send_message(..., model=model)
```

**Savings**: 83% cost reduction for ~40% of requests (simple queries).

---

#### 4.2 Local Tool Execution (Eliminate Unnecessary API Calls)

**Current**: Every tool call requires API roundtrip.

**Optimized**: Execute deterministic tools locally, only call API for decisions.

**Example**:
```python
# Current flow (3 API calls):
User: "What's the time and list inbox tasks"
API Call 1: LLM decides to call get_current_time() and list_tasks()
API Call 2: Execute tools, send results back
API Call 3: LLM formats results

# Optimized flow (1 API call):
User: "What's the time and list inbox tasks"
Local: Detect intent, execute get_current_time() and list_tasks() locally
API Call 1: Send results to LLM for formatting only
```

**Implementation**:
```python
def _local_tool_executor(user_message: str, agent) -> dict:
    """Execute deterministic tools locally before calling API."""

    # Simple intent detection
    if "time" in user_message.lower():
        time_result = agent.get_current_time()
        return {"get_current_time": time_result}

    if "list" in user_message.lower() and "inbox" in user_message.lower():
        tasks_result = agent.list_tasks(project_name="Inbox")
        return {"list_tasks": tasks_result}

    return {}  # No local execution possible
```

**Savings**: ~1,000-2,000 tokens per avoided API call.

---

#### 4.3 Batch Operations (Reduce API Calls)

**Current**: Process inbox one task at a time.

**Optimized**: LLM makes all decisions in one call, then batch execute.

**Implementation**:
```python
def batch_process_inbox(self, decisions: list[dict]) -> str:
    """
    Process multiple inbox tasks in one operation.

    Args:
        decisions: [
            {"task_id": "123", "action": "make_actionable", "location": "home", "activity": "chore"},
            {"task_id": "456", "action": "ask_question", "person": "jane"},
            {"task_id": "789", "action": "delete"}
        ]
    """
    results = []
    for decision in decisions:
        task_id = decision.pop("task_id")
        action = decision.pop("action")

        # Execute action
        method = getattr(self, action)
        result = method(task_id=task_id, **decision)
        results.append(result)

    return self._success({
        "processed": len(results),
        "results": results
    })
```

**Current inbox processing**: 46 tasks √ó 2 API calls = **92 API calls**
**Optimized**: 1 planning call + 1 execution call = **2 API calls**

**Savings**: ~90,000 tokens for full inbox processing.

---

### ‚ö´ TIER 5: Future-Proofing (Ongoing)

#### 5.1 Token Usage Monitoring & Alerting

**Implementation**:
```python
# In core/main.py
def display_token_usage(session_tokens: dict, agent_name: str, model: str):
    """Display usage and alert if exceeding thresholds."""

    # ... existing display code ...

    # Alert on anomalies
    if session_tokens["total"] > 15000:
        console.print("[red]‚ö†Ô∏è  HIGH TOKEN USAGE: {session_tokens['total']} tokens[/red]")
        console.print("[yellow]Consider using a more focused agent or compressing data[/yellow]")

    # Track historical usage
    _log_token_usage(agent_name, model, session_tokens)

def _log_token_usage(agent_name: str, model: str, tokens: dict):
    """Log token usage to file for analysis."""
    log_file = Path("logs/token_usage.jsonl")
    log_file.parent.mkdir(exist_ok=True)

    with open(log_file, "a") as f:
        f.write(json.dumps({
            "timestamp": datetime.now().isoformat(),
            "agent": agent_name,
            "model": model,
            **tokens
        }) + "\n")
```

**Analysis Script**:
```python
#!/usr/bin/env python3
"""Analyze token usage patterns."""

import json
from pathlib import Path
from collections import defaultdict

def analyze_usage():
    """Analyze historical token usage."""

    usage_by_agent = defaultdict(lambda: {"total": 0, "calls": 0})

    for line in Path("logs/token_usage.jsonl").read_text().splitlines():
        data = json.loads(line)
        agent = data["agent"]
        usage_by_agent[agent]["total"] += data["total"]
        usage_by_agent[agent]["calls"] += data["turns"]

    # Report
    print("Token Usage by Agent:")
    for agent, stats in usage_by_agent.items():
        avg = stats["total"] / stats["calls"] if stats["calls"] > 0 else 0
        print(f"  {agent}: {stats['total']:,} tokens across {stats['calls']} calls (avg: {avg:.0f})")

if __name__ == "__main__":
    analyze_usage()
```

---

#### 5.2 Automated Optimization Tests

**Create regression tests** to ensure optimizations don't break functionality:

```python
# tests/test_token_optimization.py

def test_task_compression_preserves_data():
    """Ensure compressed tasks contain all critical fields."""
    agent = TodoistAgent(config)

    # Get full task
    full_task = agent._get_task("123")

    # Compress
    compressed = agent._compress_task(full_task)

    # Verify critical fields
    assert compressed["id"] == full_task.id
    assert compressed["txt"] == full_task.content
    assert compressed["lbl"] == full_task.labels

def test_token_budget_not_exceeded():
    """Ensure startup doesn't exceed token budget."""
    agent = TodoistAgent(config)
    provider = get_provider("openai")

    # Generate schemas
    tools = provider.format_tool_schemas(agent)

    # Calculate tokens (rough estimate)
    total_tokens = sum(len(json.dumps(t)) // 4 for t in tools)  # ~4 chars per token

    assert total_tokens < 3000, f"Tool schemas too large: {total_tokens} tokens"
```

---

#### 5.3 Cost Budgeting & Limits

**Implement spending caps**:

```python
# config.yaml
cost_controls:
  daily_budget_usd: 5.00
  alert_threshold_usd: 4.00
  model_fallback: gpt-4o-mini

# In core/main.py
def check_budget(model: str, estimated_tokens: int) -> bool:
    """Check if request is within budget."""

    cost_per_token = {
        "gpt-4o": 0.0000025,  # $2.50 per 1M tokens
        "gpt-4o-mini": 0.00000015,  # $0.15 per 1M tokens
        "claude-sonnet-4.5": 0.000003  # $3.00 per 1M tokens
    }

    estimated_cost = estimated_tokens * cost_per_token[model]

    today_spent = _get_daily_spending()
    daily_budget = 5.00  # From config

    if today_spent + estimated_cost > daily_budget:
        console.print(f"[red]‚ö†Ô∏è  Budget exceeded: ${today_spent + estimated_cost:.2f} / ${daily_budget}[/red]")
        return False

    return True
```

---

## Implementation Roadmap

### Phase 2: Immediate Wins (This Week)
**Estimated Time**: 3-4 hours
**Estimated Savings**: 3,500-5,000 tokens (25-35% additional reduction)

- [ ] **Day 1**: Compress tool docstrings (Tier 1.1)
- [ ] **Day 2**: Implement task data compression (Tier 1.2)
- [ ] **Day 3**: Create specialized agent variants (Tier 1.3)
- [ ] **Day 4**: Test and validate all changes

**Expected outcome**: ~7,000-8,000 tokens per interaction (down from ~12,000)

---

### Phase 3: Medium-Term (Next 2 Weeks)
**Estimated Time**: 8-10 hours
**Estimated Savings**: 1,500-2,500 tokens (10-15% additional reduction)

- [ ] **Week 1**: Optimize prompt caching (Tier 2.1)
- [ ] **Week 1**: Implement knowledge base indexing (Tier 2.2)
- [ ] **Week 2**: Add message history summarization (Tier 2.3)
- [ ] **Week 2**: Implement streaming responses (Tier 3.1)

**Expected outcome**: ~5,500-6,500 tokens per interaction

---

### Phase 4: Long-Term (Next Month)
**Estimated Time**: 15-20 hours
**Estimated Savings**: 2,000-4,000 tokens (20-30% additional reduction)

- [ ] **Week 1-2**: Multi-model routing (Tier 4.1)
- [ ] **Week 2-3**: Local tool execution (Tier 4.2)
- [ ] **Week 3-4**: Batch operations (Tier 4.3)
- [ ] **Week 4**: Token monitoring & alerting (Tier 5.1-5.3)

**Expected outcome**: ~3,500-4,500 tokens per interaction + 40% of requests on gpt-4o-mini

---

## Cost Impact Projection

### Current Costs (After Phase 1)

**Assumptions**:
- 50 interactions/day
- Average 8,000 tokens/interaction (with task data)
- Model: gpt-4o ($2.50 per 1M input tokens)

**Monthly cost**:
- Tokens: 50 √ó 8,000 √ó 30 = 12M tokens
- Cost: 12M √ó $0.0000025 = **$30/month**

---

### After Full Optimization (Phase 2-4)

**Assumptions**:
- 50 interactions/day
- Average 4,000 tokens/interaction (optimized)
- 40% on gpt-4o-mini, 60% on gpt-4o

**Monthly cost**:
- gpt-4o-mini: 20 √ó 4,000 √ó 30 = 2.4M tokens ‚Üí 2.4M √ó $0.00000015 = **$0.36**
- gpt-4o: 30 √ó 4,000 √ó 30 = 3.6M tokens ‚Üí 3.6M √ó $0.0000025 = **$9.00**
- **Total: $9.36/month** (69% reduction from $30)

---

### At Scale (1000 interactions/day)

**Current**: 1000 √ó 8,000 √ó 30 = 240M tokens ‚Üí **$600/month**

**Optimized**:
- gpt-4o-mini: 400 √ó 4,000 √ó 30 = 48M tokens ‚Üí **$7.20**
- gpt-4o: 600 √ó 4,000 √ó 30 = 72M tokens ‚Üí **$180**
- **Total: $187.20/month** (69% reduction, **$412.80 saved/month**)

**Annual savings at scale**: **$4,953.60**

---

## Quick Reference: Optimization Checklist

### ‚úÖ Completed
- [x] Extract Literal type enums (Phase 1)
- [x] Compress system prompt (Phase 1)
- [x] Enable prompt caching (Phase 1)

### üéØ High Priority (Do This Week)
- [ ] Compress tool docstrings (600 tokens)
- [ ] Implement task data compression (2,300-4,600 tokens)
- [ ] Create specialized agent variants (2,200 tokens)

### üìÖ Medium Priority (Next 2 Weeks)
- [ ] Knowledge base indexing (300-600 tokens)
- [ ] Message history summarization (500-1,000 tokens)
- [ ] Streaming responses (perceived performance)

### üîÆ Future Enhancements
- [ ] Multi-model routing (83% cost reduction for simple queries)
- [ ] Local tool execution (eliminate unnecessary API calls)
- [ ] Batch operations (90% fewer API calls for inbox)
- [ ] Token monitoring & alerting

---

## Monitoring & Validation

### Key Metrics to Track

1. **Tokens per interaction** (target: <4,000)
2. **Cache hit rate** (target: >95%)
3. **API calls per workflow** (target: <5 for inbox processing)
4. **Monthly cost** (target: <$10 for current usage)
5. **User satisfaction** (ensure optimizations don't degrade experience)

### Validation Tests

```bash
# Test token usage after each optimization
./synapse to
> list inbox tasks
# Check: "Tokens: XXX in + YYY out = ZZZ total"
# Target: <3,500 total

# Test functionality unchanged
./synapse to
> capture Test task
> list inbox
> d
> exit
# All commands should work identically

# Regression test suite
python -m pytest tests/test_token_optimization.py
```

---

## Critical Success Factors

### ‚úÖ Do's
1. **Test after each optimization** - Don't batch changes
2. **Monitor cache hit rate** - It's your biggest savings
3. **Preserve functionality** - Never sacrifice UX for tokens
4. **Track costs** - Log usage to logs/token_usage.jsonl
5. **Use specialized agents** - Route to right tool set

### ‚ùå Don'ts
1. **Don't over-compress** - LLM needs enough context
2. **Don't skip validation** - Broken functionality costs more than tokens
3. **Don't optimize prematurely** - Focus on high-impact changes first
4. **Don't ignore user feedback** - Perceived speed matters
5. **Don't forget to commit** - Git saves your optimizations

---

## Emergency Rollback Plan

If optimization breaks functionality:

```bash
# View recent changes
git log --oneline -10

# Check specific optimization commit
git show <commit-hash>

# Revert specific file
git checkout HEAD~1 -- core/agents/todoist.py

# Full rollback to before optimizations
git reset --hard <commit-before-optimizations>

# Or use git revert for clean history
git revert <bad-commit-hash>
```

---

## Conclusion

Your Synapse project has **massive** token optimization potential:

- **Phase 1** (complete): 38% reduction ‚úÖ
- **Phase 2** (quick wins): +25-35% reduction üéØ
- **Phase 3-4** (medium/long-term): +30-45% reduction üîÆ

**Total potential**: **70-80% reduction** from original baseline.

**ROI**: At scale (1000 interactions/day), this could save **~$5,000/year** while maintaining or improving user experience.

**Next Steps**:
1. Start with Tier 1 optimizations (highest ROI)
2. Test extensively (don't break functionality)
3. Monitor token usage (logs/token_usage.jsonl)
4. Iterate based on real-world patterns

The key is to **optimize incrementally** and **validate continuously**. Each small improvement compounds over time.

---

**Last Updated**: 2025-10-18
**Status**: Ready to implement Phase 2
**Owner**: Synapse AI Orchestration Engine
